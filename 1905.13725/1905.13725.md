# Are Labels Required For Improving Adversarial Robustness?

Jonathan Uesato∗ Jean-Baptiste Alayrac∗ **Po-Sen Huang**∗
Robert Stanforth Alhussein Fawzi Pushmeet Kohli DeepMind
{juesato,jalayrac,posenhuang}@google.com

## Abstract

Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that *unlabeled* data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7% over using 4K supervised examples alone, and captures over 95% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4% over the previous state-of-theart on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.

## 1 Introduction

Deep learning has revolutionized many areas of research such as natural language processing, speech recognition or computer vision. System based on these techniques are now being developed and deployed for a wide variety of applications, from recommending/ranking content on the web [13, 22] to autonomous driving [7] and even in medical diagnostics [14]. The safety-critical nature of some of these tasks necessitates the need for ensuring that the deployed models are robust and generalize well to all sorts of variations that can occur in the inputs. Yet, it has been shown that the commonly used deep learning models are vulnerable to adversarial perturbations in the input [43], *e.g.* it is possible to fool an image classifier into predicting arbitrary classes by carefully choosing perturbations imperceptible to the human eye.

Since the discovery of these results, many approaches have been developed to prevent this type of behaviour. One of the most effective and popular approaches is known as supervised adversarial training [17, 30] which works by generating adversarial samples in an online manner through an inner optimization procedure and then using them to augment the standard training set. Despite substantial work in this space, accuracy of classifiers on adversarial inputs remains much lower than that on normal inputs. Recent theoretical work has offered a reason for this discrepancy, and argues
*Equal contribution, random order.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.
that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for the standard classification task [41].

This result is a key hurdle in the development and deployment of robust machine learning models in many real world applications where labeled data is expensive. Our central hypothesis is that additional unlabeled examples may suffice for adversarial robustness. Intuitively, this is based on two related observations, explained in Sections 3 and 4.1.2. First, adversarial robustness depends on the smoothness of the classifier around natural images, which can be estimated from unlabeled data.

Second, only a relatively small amount of labeled data is needed for standard generalization. Thus, if adversarial training is robust to label noise, labels from supervised examples can be propagated to unsupervised examples to train a smoothed classifier with improved adversarial robustness.

Motivated by this, we explore Unsupervised Adversarial Training (UAT) to use unlabeled data for adversarial training. We study this algorithm in a simple theoretical setting, proposed by [41] to study adversarial generalization. We show that once we are given a single labeled example, the sample complexity of UAT matches the fully supervised case up to constant factors. In independent and concurrent work, [10, 53, 33] also study the use of unlabeled data for improving adversarial robustness, which we discuss in Section 2.

Experimentally, we find strong support for our main hypothesis. On CIFAR-10 and SVHN, with very limited annotated data, our method reaches robust accuracies of 54.1% and 84.4% respectively against the FGSM20 attack [26]. These numbers represent a significant improvement over purely supervised approaches (32.5% and 66.0%) on same amount of data and almost match methods that have access to full supervision (55.5% and 86.2%), capturing over 95% of their improvement, without labels. Further, we show that we can successfully leverage realistically obtained unsupervised and uncurated data to improve the state-of-the-art on CIFAR-10 at ε = 8/255 from 52.58% to 56.30%
against the strongest known attack.

Contributions. (i) In Section 3, we propose a simple and theoretically grounded strategy, UAT, to leverage unsupervised data for adversarial training. **(ii)** We provide, in Section 4.1, strong empirical support for our initial hypothesis that *unlabeled data can be competitive with labeled data when it* comes to training adversarially robust classifiers, therefore opening a new avenue for improving adversarial training. **(iii)** Finally, by leveraging noisy and uncurated data obtained from web queries, we set a new state-of-the-art on CIFAR-10 without depending on any additional human labeling.

## 2 Related Work

Adversarial Robustness. [5, 43] observed that neural networks which achieve extremely high accuracy on a randomly sampled test set may nonetheless be vulnerable to adversarial examples, or small but highly optimized perturbations of the data which cause misclassification. Since then, many papers have proposed a wide variety of defenses to the so-called adversarial attacks, though few have proven robust against stronger attacks [1, 8, 46]. One of the most successful approaches for obtaining classifiers that are adversarially robust is adversarial training [1, 46]. Adversarial training directly minimizes the adversarial risk by approximately solving an inner maximization problem by projected gradient descent (PGD) to generate small perturbations that increase the prediction error, and uses these perturbed examples for training [17, 26, 30]. The TRADES approach in [54]
improves these results by instead minimizing a surrogate loss which upper bounds the adversarial risk. Their objective is very similar to the one used in UAT-OT (UAT with online targets introduced in Section 3.1) but is estimated purely on labeled rather than unlabeled data.

Common to all these approaches, a central challenge is adversarial generalization. For example, on CIFAR-10 with a perturbation of ε = 8/255, the adversarially trained model in [30] achieves an adversarial accuracy of 46%, despite near 100% adversarial accuracy on the train set. For comparison, standard models can achieve natural accuracy of 96% on CIFAR-10 [52]. Several recent papers have studied generalization bounds for adversarial robustness [2, 23, 41, 51]. Of particular relevance to our work, [41] argues that adversarial generalization may require more data than natural generalization.

One solution explored in [21] is to use pretraining on ImageNet, a large supervised dataset, to improve adversarial robustness. In this work, we study whether more labeled data is necessary, or whether unlabeled data can suffice. While to our knowledge, this question has not been directly studied, several works such as [20, 40, 42] propose using generative models to detect or denoise adversarial examples, which can in principle be learned on unlabeled data. However, so far, such approaches have not proven to be robust to strong attacks [1, 46].

Semi-supervised learning. Learning from unlabeled data is an active area of research. The semisupervised learning approach [11] which, in addition to labeled data, also uses unlabeled data to learn better models is particularly relevant to our work. One of the most effective technique for semi-supervised learning is to use smoothness regularization: the model is trained to be invariant to small perturbation applied to unsupervised samples [3, 4, 27, 32, 39, 50]. Of particular relevance to UAT, [32] also uses adversarial perturbations to smooth the model outputs. In addition, co-training
[6] and recent extensions [12, 37] use the most confident predictions on unlabeled data to iteratively construct additional labeled training data. These work all focus on improving standard generalization whereas we explore the use of similar ideas in the context of adversarial generalization.

Semi-supervised learning for adversarial robustness. The observation that adversarial robustness can be optimized without labels was made independently and concurrently by [10, 33, 53]. Of particular interest, [10] proposes a meta-algorithm Robust Self-Training (RST), similar to UAT.

Indeed, the particular instantiation of RST used in [10] and the fixed-target variant of UAT are nearly equivalent: the difference is whether the base algorithm minimizes the robust loss from [54] or the vanilla adversarial training objective [30]. Their results also provide strong, independent evidence that unlabeled and uncurated examples improve robustness on both CIFAR-10 and SVHN.

## 3 Unsupervised Adversarial Training (Uat)

In this section, we introduce and motivate our approach, Unsupervised Adversarial Training (UAT),
which enables the use of unlabeled data to train robust classifiers.

Notation. Consider the classification problem of learning a predictor fθ to map inputs x ∈ X to labels y ∈ Y. In this work, f is of the form: fθ(x) = arg maxy∈Y pθ(y|x), where pθ(.|x) is parameterized by a neural network. We assume data points (*x, y*) are i.i.d. samples from the data-generating joint distribution P(*X, Y* ) over *X × Y*. P(X) denotes the unlabeled distribution over X obtained by marginalizing out Y in P(*X, Y* ). We assume access to a labeled training set Sn = {(xi, yi)}1≤i≤n, where (xi, yi) ∼ P(*X, Y* ) and an unlabeled training set Um = {xi}1≤i≤m, where xi ∼ P(X).

Evaluation of Adversarial Robustness. The natural risk is Lnat(θ) = E(x,y)∼P (X,Y ) `(*y, f*θ(x)),
where ` is the 0−1 loss. Our primary objective is minimizing adversarial risk: Ladv(θ) =
EP (X,Y ) supx0∈N(x)`(*y, f*θ(x 0)). As is common, the neighborhood N(x) is taken in this work to be the L∞ ball: N(x) = {x 0: kx 0 − xk∞ ≤ }. Because the inner maximization cannot be solved exactly, we report the surrogate adversarial risk Lg(θ) = EP (X,Y ) `(fθ(x 0), y), where x 0 = g(*x, y, θ*)
is an approximate solution to the inner maximization computed by some fixed adversary g. Typically, g is (a variant of) projected gradient descent (PGD) with a fixed number of iterations.

## 3.1 Unsupervised Adversarial Training (Uat)

Motivation. As discussed in the introduction, a central challenge for adversarial training has been the difficulty of adversarial generalization. Previous work has argued that adversarial generalization may simply require more data than natural generalization. We ask a simple question: is more *labeled* data necessary, or is *unsupervised* data sufficient? This is of particular interest in the common setting where unlabeled examples are dramatically cheaper to acquire than labeled examples (m  n).

For example, for large-scale image classification problems, unlabeled examples can be acquired by scraping images off the web, whereas gathering labeled examples requires hiring human labelers.

We now consider two algorithms to study this question. Both approaches are simple - we emphasize the point that large unlabeled datasets can help bridge the gap between natural and adversarial generalization. Later, in Sections 3.2 and 4, we show that both in a simple theoretical model and empirically, unlabeled data is in fact *competitive* with labeled data. In other words, for a fixed number of additional examples, we observe similar improvements in adversarial robustness regardless of whether or not they are labeled.

Strategy 1: Unsupervised Adversarial Training with Online Targets (UAT-OT). We note that adversarial risk can be bounded as Ladv = Lnat + (Ladv − Lnat) ≤ Lnat +
EP (X,Y ) supx0∈N(x)`(fθ(x 0), fθ(x)), similarly to the decomposition in [54]. We refer to the first term as the *classification* loss, and the second terms as the *smoothness* loss. Even for adversarially trained models, it has been observed that the smoothness loss dominates the classification loss on the test set, suggesting that controlling the smoothness loss is the key to adversarial generalization. For example, the adversarially trained model in [30] achieves natural accuracy of 87% but adversarial accuracy of 46% on CIFAR-10 at ε = 8/255.

Notably, the smoothness loss has no dependence on labels, and thus can be minimized purely through unsupervised data. UAT-OT directly minimizes a differentiable surrogate of the smoothness loss on the unlabeled data. Formally, we use the loss introduced in [32] and also used in [54]

$${\mathcal{L}}_{\mathrm{unsp}}^{O T}(\theta)=\operatorname*{\mathbb{E}}_{x\sim P(X)}\operatorname*{sup}_{x^{\prime}\in{\mathcal{N}}_{\epsilon}(x)}{\mathcal{D}}(p_{\hat{\theta}}(.|x),p_{\theta}(.|x^{\prime})),$$
$$(1)$$

where D is the Kullback-Leibler divergence, and ˆθ indicates a fixed copy of the parameters θ in order to stop the gradients from propagating. While [32], which primarily focuses on natural generalization, uses a single step approximation of the inner maximization, we use an iterative PGD adversary, since prior work indicates strong adversaries are crucial for effective adversarial training [26, 30].

Strategy 2: Unsupervised Adversarial Training with Fixed Targets (UAT-FT). This strategy directly leverages the gap between standard generalization and adversarial generalization. The main idea is to first train a *base classifier* for standard generalization on the supervised set Sn. Then, this model is used to estimate labels, hence *fixed targets*, on the unsupervised set Um. This allows us to employ standard supervised adversarial training using these fixed targets. Formally, it corresponds to using the following loss:

$${\mathcal{L}}_{\mathrm{unsp}}^{F T}(\theta)=\operatorname*{\mathbb{E}}_{x\sim P(X)}\operatorname*{sup}_{x^{\prime}\in{\mathcal{N}}_{e}(x)}\mathtt{xent}({\hat{y}}(x),p_{\theta}(.|x^{\prime})),$$
$$(2)$$

where xent is the cross entropy loss and yˆ(x) is a *pseudo*-label obtained from a model trained for standard generalization on Sn alone. Thus, provided a sufficiently large unlabeled dataset, UAT-FT
recovers a smoothed version of the base classifier, which matches the predictions of the base classifier on clean data, while maintaining stability of the predictions within local neighborhoods of the data.

Overall training. For the overall objective, we use a weighted combination of the supervised loss and the chosen unsupervised loss, controlled by a hyperparameter λ: L(θ) = Lsup(θ) + λLunsup(θ).

The unsupervised loss can be either L
OT
unsup (UAT-OT), L
FT
unsup (UAT-FT) or both (UAT++). Finally, note that the unsupervised loss can also be used on the samples of the supervised set by simply adding the xi's of Sn in Um. The pseudocode and implmenetation details are described in Appendix A.1.

## 3.2 Theoretical Model

To improve our understanding of the effects of unlabeled data, we study the simple setting proposed by [41] to analyze the required sample complexity of adversarial robustness.

Definition 1 (Gaussian model [41]). Let θ
∗ ∈ R
d be the per-class mean vector and let σ > 0 be the variance parameter. Then the (θ
∗, σ)*-Gaussian model is defined by the following distribution over*
(*x, y*) ∈ R
d × {±1}: First, draw a label y ∈ {±1} *uniformly at random. Then sample the data point* x ∈ R
d*from* N (y · θ
∗, σ2I).

In [41], this setting was chosen to model the empirical observation that adversarial generalization requires more data than natural generalization. They provide an algorithm which achieves fixed, arbitrary (say, 1%) accuracy using a single sample. However, to achieve the same adversarial accuracy, they show that any algorithm requires at least c1ε 2√d / log d samples and provide an algorithm requiring n ≥ c2ε 2√d samples, for fixed constants c1, c2.

Here, we show that this sample complexity can be dramatically improved by replacing labeled examples with unlabeled samples. We first define an analogue of UAT-FT to leverage unlabeled data in this setting. For training an adversarially robust classifier, the algorithm in [41] computes a sample mean of per-point estimates. We straightforwardly adapt this procedure for unlabeled data, as in UAT-FT: we first estimate a base classifier from the labeled examples, then compute a sample mean using fixed targets from this base classifier.

Definition 2 (Gaussian UAT-FT). Given n labeled examples (x1, y1), . . . ,(xn, yn) and m *unlabeled* examples xn+1, . . . , xn+m, let wˆsup denote the sample mean estimator on labeled examples: wˆsup = Pn i=1 yixi. The UAT-FT estimator is then defined as the sample mean wˆ =Pn+m i=n+1 yˆixi *where* yˆi = fwˆsup
(xi).

Theorem 1 states that in contrast to the purely supervised setting which requires O(
√d / log d )
examples, in the semi-supervised setting, a single labeled example, along with O(
√d ) examples are sufficient to achieve fixed, arbitrary accuracy.

Theorem 1. *Consider the* (θ
∗, σ)*-Gaussian model with* kθ
∗k2 =
√d and σ ≤
1 32 d 1/4. Let wˆ be the the UAT-FT estimator as in Definition *2. Then with high probability, for* n = 1*, the linear classifier* fwˆ has `
∞*-robust classification error at most* 1% if m ≥ c2√d where c is a fixed, universal constant. The proof is deferred to Appendix G. For ease of comparison, we consider the same Gaussian model parameters kθ
∗k2 and σ as used in [41]. The sample complexity in Theorem 1 matches the sample complexity of the algorithm provided in [41] up to constant factors, despite using unlabeled rather than labeled examples. We now turn to empirical investigation of whether this result is reflected in practical settings.

## 4 Experiments

In section 4.1, we first investigate our primary question: for adversarial robustness, can unlabeled examples be competitive with labeled examples? These operate in the standard semi-supervised setting where we use a small fraction of the original training set as Sn, and provide varying amounts of the remainder as Um. After observing high robustness, particularly for UAT-FT and UAT++, we run several controlled experiments in section 4.1.2 to understand why this approach works well. In section 4.2, we explore the robustness of UAT to shift in the distribution P(X). Finally, we use UAT
to improve existing state-of-the-art adversarial robustness on CIFAR-10, using the 80 Million Tiny Images dataset as our source of unlabeled data.

## 4.1 Adversarial Robustness With Few Labels

Experimental setup. We run experiments on the CIFAR-10 and SVHN datasets, with L∞ constraints of ε = 8/255 and ε = 0.01 respectively, which are standard for studying adversarial robustness of image classifiers [18, 30, 54, 48]. For adversarial evaluation, we report against 20-step iterative FGSM [26], for consistency with previous state-of-the-art [54]. In our later experiments for Section 4.2.2, we also evaluate against a much stronger attack, MultiTargeted [19], which provides a more accurate proxy for the adversarial risk. As we demonstrate in Appendix E.1, the MultiTargeted attack is significantly stronger than an expensive PGD attack with random restarts, which is in turn significantly stronger than FGSM20. We follow previous work [30, 54] for our choices of model architecture, data preprocessing, and hyperparameters, which are detailed in Appendix A.

To study the effect of unlabeled data, we randomly split the existing training set into a small supervised set Sn and use the remaining N − n training examples as a source of unlabeled data. We also split out 10000 examples from the training set to use as validation, for both CIFAR-10 and SVHN, since neither dataset comes with a validation set. We then study the effect on robust accuracy of increasing m, the number of unsupervised samples, across different regimes (m ≈ n vs. m  n).

Baselines. We compare results with the two strongest existing supervised approaches, standard adversarial training [30] and TRADES [54], which do not use unsupervised data. We also compare to VAT [32], which was designed for *standard* semi-supervised learning but can be adapted for unsupervised adversarial training as explained in Appendix B. Finally, to compare the benefits of labeled and unlabeled data, we compare to the *supervised oracle*, which represents the best possible performance, where the model is provided the ground-truth label even for samples from Um.

## 4.1.1 Main Results

We first test the hypothesis that for adversarial robustness, additional unlabeled data is competitive with additional labeled data. Figure 1 summarizes the results. We report the adversarial accuracy for varying m, when n is fixed to 4000 and 1000, for CIFAR-10 and SVHN respectively.

Comparison to baselines. All models show significant improvements in adversarial robustness over the baselines for all numbers of unsupervised samples. With the maximum number (32k / 60k) of unlabeled images, even the weakest UAT model, UAT-OT, shows 12.9% / 16.9% improvement over

![5_image_0.png](5_image_0.png) 

Figure 1: Comparison of labeled data and unsupervised data for improving adversarial generalization on CIFAR-10 (**left,a**) and SVHN (**right,b**)
the baselines not leveraging unlabeled data, and 6.4% / 1.6% improvement over VAT on CIFAR-10 and SVHN, respectively.

Comparison between UAT variants. We compare the results of 3 different UAT variants: UATOT, UAT-FT, and UAT++. Comparing UAT-FT and UAT-OT, when there are larger number of unsupervised samples, we observe that the UAT-FT shows a significant improvement compared to UAT-OT on CIFAR-10, UAT-OT performs similarly to UAT-FT on SVHN. With smaller numbers of unsupervised samples, the two approaches perform similarly. Empirically, we observe that UAT++,
which combines the two approaches, outperforms either individually. We thus primarily use UAT++
for our later experiments.

Comparison to the oracle. Figure 1 provides strong support for our main hypothesis. In particular, we observe that when using large unsupervised datasets, UAT++ performs nearly as well as the supervised oracle. In Fig. 1a, with 32K unlabeled examples, UAT++ achieves 54.1% on CIFAR-10, which is 1.4% lower than the supervised oracle. Similarly, with 60K unlabeled data, in Fig. 1b, UAT++ achieves 84.4% on SVHN which is 1.8% lower than the supervised oracle.

Conclusion. We demonstrate that, leveraging large amounts of unlabeled examples, UAT++ achieves similar adversarial robustness to supervised oracle, which uses label information. In particular, without requiring labels, UAT++ captures over 97.6% / 97.9% of the improvement from 32K / 60K
additional examples compared with supervised oracle on CIFAR-10 and SVHN, respectively.

## 4.1.2 Label Noise Analysis

Given the effectiveness of UAT-FT and UAT++, we perform an ablation study on the impact of label noise on UAT for adversarial robustness.

Experimental setup. To do so, we first divide the CIFAR-10 training set into halves, where the first 20K examples are used for training the base classifier and the latter 20K are used to train a UAT
model. Of the latter 20K, we treat 4K examples as labeled, as in Section 4.1.1, and the remaining 16K as unlabeled. We consider two different approaches to introducing label noise. For UAT-FT
(Correlated), we produce pseudo-labels using the UAT-FT procedure, where the number of training examples used for the base classifier varies between between 500 and 20K. This produces base classifiers with error rates between 7% and 48%. For UAT-FT (Random), we randomly flip the label to a randomly selected incorrect class. The results are shown in Figure 2.

Analysis. In Fig. 2a, in the UAT-FT (Random) case, adversarial accuracy is relatively flat between 1% and 20%. Even with 50% of the examples mislabeled, the decrease in robust accuracy is less than 10%. At the highest level of noise, UAT-FT still obtains a 8.0% improvement in robustness accuracy over the strongest baseline which does not exploit unsupervised data. Similarly, in the UAT-FT (Correlated) case, robust accuracy is relatively flat between 7% and 23% noise level, and even at 48% corrupted labels, UAT-FT outperforms the purely supervised baselines by 6.3%.

To understand these results, we believe that the main function of the unsupervised data in UAT is to improve generalization of the smoothness loss, rather than the classification loss. While examples with

![6_image_0.png](6_image_0.png)

Figure 2: Effects of label noise on adversarial **(left, a**) and natural **(right, b)** accuracies, on CIFAR-10
corrupted labels have limited utility for improving classification accuracy, they can still be leveraged to improve the smoothness loss. This is most obvious in UAT-OT, which has no dependence on the predicted labels (and is thus a flat line in Figure 2a). However, Figure 2a supports the hypothesis that UAT-FT also works similarly, given its effectiveness even in cases where up to half of the labels are corrupted. As mentioned in Section 3, because generalization gap of the classification loss is typically already small, controlling generalization of the smoothness loss is key to improved adversarial robustness.

Comparison to standard generalization. We compare the robustness of UAT-FT to label noise, to an analogous pseudo-labeling technique applied to natural generalization. Comparing between Figures 2a and 2b, we observe that with increasing label noise, the rate of degradation in robustness of adversarial trained models is much lower than the rate of degradation in accuracy of models obtained with standard training. In particular, while standard training procedures can be robust to random label noise, as observed in previous work [36, 38], accuracy decreases almost one-to-one (slope -0.78)
with correlated errors. This is natural, as with a very large unsupervised dataset, we expect to recover the base classifier (modulo the 4k additional supervised examples).

Conclusion. UAT shows significant robustness to label noise, achieving an 8.0% improvement over the baseline even with nearly 50% error in the base classifier. We hypothesize that this is primarily because UAT operates primarily on the smoothness loss, rather than the classification loss, and is thus less dependent on the pseudo-labels.

## 4.2 Unsupervised Data With Distribution Shift

Motivation. In Section 4.1, we studied the standard semi-supervised setting, where P(X) is the marginal of the joint distribution P(*X, Y* ). As pointed out in [35], real-world unlabeled datasets may involve varying degrees of distribution shift from the labeled distribution. For example, images from CIFAR-10, even without labels, required human curation to not only restrict to images of the choosen 10 classes but also to ensure that selected images were photo-realistic (line drawings were rejected) or that only one instance of the object was present (see Appendix C of [25] for the full labeler instruction sheet). We thus study whether our approach is robust to such distribution shift, allowing us to fully leverage data which is not only unlabeled, but also uncurated.

We use the **80 Million Tiny Images** [44] dataset (hereafter, 80m) as our uncurated data source, a large dataset obtained by web queries for 75,062 words. Because collecting this dataset required no human filtering, it provides a perfect example of uncurated data that is cheaply available at scale.

Notably, CIFAR-10 is a human-labeled subset of 80m, which has been restricted to 10 classes.

Preprocessing. Because the majority of 80m contains images distinct from the CIFAR-10 classes, we apply an automated filtering technique similar to [50], detailed in Appendix C. Briefly, we first restrict to images obtained from web queries matching the CIFAR-10 classes, and filter out near duplicates of the CIFAR-10 test set using GIST features [34, 15]. For each class, we rank the images based on the prediction confidence from a WideResNet-28-10 model pretrained on the CIFAR-10 dataset.

We then take the top 10k, 20k, or 50k images per class, to create the 80m@100K, 80m@200K, and 80m@500K datasets, respectively.

Method Sup. Data Unsup. Data Network Anat AF GSM20 A*MultiT ar.*

[48] CIFAR-10 ✗ - 27.07% 23.54% -

AT [30] CIFAR-10 ✗ WRN-28 87.30% 47.04% 44.54%

[55] CIFAR-10 ✗ - 94.64% 0.15% - [26] CIFAR-10 ✗ - 85.25% 45.89% -

[21] ImageNet + CIFAR-10 ✗ WRN-28 87.1% 57.40% ≤52.9%*

AT-Reimpl. [30] CIFAR-10 ✗ WRN-34 87.08% 52.93% 47.10%

TRADES [54] CIFAR-10 ✗ WRN-34 84.92% 57.11% 52.58%

UAT++ CIFAR-10 80m@100K WRN-34 86.04% 59.41% 52.64%

UAT++ CIFAR-10 80m@200K WRN-34 85.85% **62.18% 53.35%**

UAT++ CIFAR-10 80m@500K WRN-34 78.34% 58.04% 48.99%

UAT++ CIFAR-10 80m@200K WRN-70 86.75% 62.89% 55.04%

UAT++ CIFAR-10 80m@200K WRN-106 86.46% **63.65% 56.30%**

Table 1: Experimental results using 80m Tiny Images dataset (as a unsupervised data) and CIFAR-10 (as

supervised data), where Anat represents the original test accuracy, A*F GSM*20 represents the adversarial accuracy

under 20 step FGSM, and A*MultiT ar.* represents the adversarial accuracy under the strong MultiTargeted

attack. WRN-k denotes the Wide-ResNet with depth k. '*' indicates it is from [21] using 100 PGD steps with

1000 random restarts, an attack that we have found to be weaker than the MultiTargeted attack.

Overview. We first conduct a preliminary study on the impact of distribution shift in a low data regime in Section 4.2.1, and we finally demonstrate how UAT can be used to leverage large scale realistic uncurated data in Section 4.2.2.

## 4.2.1 Preliminary Study: Low Data Regime

To study the effect of having unsupervised data from a different distribution, we repeat the same experimental setup described in Section 4.1.1 where we draw Um from 80m@200K rather than CIFAR-10. Results are given in Figure 3. For simplicity, we report our best performing method, UAT++, in both settings: Um ⊂ 80m@200K and Um ⊂ CIFAR-10. First, we observe that when using 32K images from unsupervised data, either UAT++ (80m@200K) or UAT++ (CIFAR-10)
outperforms the baseline, TRADES [54], which only uses the 4K supervised examples. Specifically, UAT++ with 80m@200K achieves 48.6% robust accuracy, a 16.2% improvement over TRADES. On the other hand, UAT++ performs substantially better when Um is drawn from CIFAR-10 rather than 80m@200K, by a margin of 5.5% with 32K unlabeled examples.

Conclusion. While unlabeled data from the same distribution is significantly better than off-distribution unlabeled data, the off-distribution unlabeled data is still much better than no unsupervised data at all. In the next section, we explore scaling up the off-distribution case.

## 4.2.2 Large Scale Regime

![7_image_0.png](7_image_0.png)

We now study whether uncurated data alone can be leveraged to improve the state-of-the-art for adversarial robustness on CIFAR-10. For these experiments, we use subsets of 80m in conjunction with the full CIFAR-10 training set. Table 1 summarizes the results. We report adversarial accuracies against two attacks. First, we consider the FGSM [17, 26] attack with 20 steps (FGSM20) to allow for direct comparison with previous state-of-the-art [54]. Second, we evaluate against the MultiTargeted attack, which we find to be significantly stronger than the commonly used PGD attack with random restarts. Details are provided in Appendix E.1.

Baselines. For baseline models, we evaluate the models released by [30, 54]. For fair comparison with our setup, we also reimplement adversarial training (AT-Reimpl. [30]) using the same attack we use for UAT, which we found to be slightly more efficient than the original attack. This is detailed in Appendix A.3. We also compare to [21], which uses more *labeled* data by pretraining on ImageNet.

All other reported numbers are taken from [54].

Figure 3: Distribution shift on CIFAR-10 Comparison with same model. First, we compare UAT++ with three different sets of unsupervised data (80m@100K, 80m@200K, and 80m@500K) using the same model architecture (WRN-34)
as in TRADES. In all cases, we outperform TRADES under FGSM20. When using 80m@200K,
we improve upon TRADES by 5.07% under FGSM20and 0.77% under the MultiTargeted attack.

We note the importance of leveraging more unsupervised data when going from 80m@100K to 80m@200K. However, performance degrades when using 80m@500K which we attribute to the fact that 80m@500K contains significantly more out-of-distribution images. Finally, comparing with the recent work of [21], we note that using more unsupervised data can outperform using additional supervised data for pretraining.

Further analysis. We run several additional checks against gradient masking [1, 45, 46], detailed in Appendix E. We show that a gradient-free attack, SPSA [46], does not lower accuracy compared to untargeted PGD (Appendix E.2), visualize loss landscapes (Appendix E.3), and empirically analyze attack convergence (Appendix E.4). Overall, we do not find evidence that other attacks could outperform the MultiTargeted attack.

A new state-of-the-art on CIFAR-10. Finally, when using these significantly larger training sets, we observe significant underfitting, where robust accuracy is low even on the training set. We thus also explore using deeper models. We observe that UAT++ trained on the 80m@200K unsupervised dataset using WRN-106 achieves state-of-the-art performance, +6.54% under FGSM20and +3.72%
against MultiTargeted attack, compared to TRADES [54]. Our trained model is available on our repository.1

## 5 Conclusion

Despite the promise of adversarial training, its reliance on large numbers of labeled examples has presented a major challenge towards developing robust classifiers. In this paper, we hypothesize that annotated data might not be as important as commonly believed for training adversarially robust classifiers. To validate this hypothesis, we introduce two simple UAT approaches which we tested on two standard image classification benchmarks. These experiments reveal that indeed, one can reach near state-of-the-art adversarial robustness with as few as 4K labels for CIFAR-10 (10 times less than the original dataset) and as few as 1K labels for SVHN (100 times less than the original dataset). Further, we demonstrate that our method can also be applied to uncurated data obtained from simple web queries. This approach improves the state-of-the-art on CIFAR-10 by 4% against the strongest known attack. These findings open a new avenue for improving adversarial robustness using unlabeled data. We believe this could be especially important for domains such as medical applications, where robustness is essential and gathering labels is particularly costly [16].

Acknowledgements. We would like to especially thank Sven Gowal for helping us evaluate with the MultiTargeted attack and for the loss landscape visualizations, as well as insightful discussions throughout this project. We would also like to thank Andrew Zisserman, Catherine Olsson, Chongli Qin, Relja Arandjelovic, Sam Smith, Taylan Cemgil, Tom Brown, and Vlad Firoiu for helpful ´
discussions throughout this work.

## References

[1] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. *arXiv preprint arXiv:1802.00420*, 2018. 2, 9, 16
[2] I. Attias, A. Kontorovich, and Y. Mansour. Improved generalization bounds for robust learning. In ALT,
2018. 2
[3] P. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. In *NeurIPS*, 2014. 3
[4] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. Raffel. MixMatch: A Holistic Approach to Semi-Supervised Learning. *arXiv:1905.02249*, 2019. 3
[5] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndic, P. Laskov, G. Giacinto, and F. Roli. Evasion ´
attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 387–402. Springer, 2013. 2 1https://github.com/deepmind/deepmind-research/tree/master/unsupervised_
adversarial_training
[6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 92–100. ACM, 1998. 3
[7] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. End to end learning for self-driving cars. *CoRR*,
abs/1604.07316, 2016. 1
[8] N. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods.

In *Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security*, pages 3–14. ACM, 2017.

2
[9] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy
(SP), 2017 IEEE Symposium on, pages 39–57. IEEE, 2017. 14
[10] Y. Carmon, A. Raghunathan, L. Schmidt, P. Liang, and J. C. Duchi. Unlabeled Data Improves Adversarial Robustness. In *NeurIPS*, 2019. 2, 3
[11] O. Chapelle, B. Scholkopf, and A. Zien. *Semi-Supervised Learning*. MITPress, 2009. 3
[12] D.-D. Chen, W. Wang, W. Gao, and Z.-H. Zhou. Tri-net for semi-supervised deep learning. In *IJCAI*, 2018.

3
[13] P. Covington, J. Adams, and E. Sargin. Deep neural networks for Youtube recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems, pages 191–198. ACM, 2016. 1
[14] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham, X. Glorot, B. O'Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. *Nature Medicine*, 24(9):1342, 2018. 1
[15] M. Douze, H. Jégou, H. Sandhawalia, L. Amsaleg, and C. Schmid. Evaluation of gist descriptors for webscale image search. In *Proceedings of the ACM International Conference on Image and Video Retrieval*,
page 19. ACM, 2009. 7, 15
[16] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S. Kohane. Adversarial attacks on medical machine learning. *Science*, 2019. 9
[17] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 1, 2, 8
[18] S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, R. Arandjelovic, T. Mann, and P. Kohli.

On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018. 5
[19] S. Gowal, J. Uesato, C. Qin, P.-S. Huang, T. Mann, and P. Kohli. An alternative surrogate loss for pgd-based adversarial testing. 2019. 5, 15
[20] S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014. 2
[21] D. Hendrycks, K. Lee, and M. Mazeika. Using pre-training can improve model robustness and uncertainty.

In *ICML*, 2019. 2, 8, 9
[22] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In *Proceedings of the 22nd ACM international conference on* Information & Knowledge Management, pages 2333–2338. ACM, 2013. 1
[23] J. Khim and P.-L. Loh. Adversarial risk bounds for binary classification via function transformation. arXiv preprint arXiv:1810.09519, 2018. 2
[24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*,
2014. 14
[25] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, 2009. 7, 15
[26] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial Machine Learning at Scale. In *ICLR*, 2017. 2, 4, 5, 8, 14
[27] S. Laine and T. Aila. Temporal ensembling for semi-supervised learnings. In *ICLR*, 2017. 3
[28] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302–1338, 2000. 17
[29] Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transferable adversarial examples and black-box attacks.

arXiv preprint arXiv:1611.02770, 2016. 14
[30] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. In *ICLR*, 2018. 1, 2, 3, 4, 5, 8, 12, 14
[31] G. A. Miller. Wordnet: a lexical database for english. *Communications of the ACM*, 38(11):39–41, 1995.

15
[32] T. Miyato, S. ichi Maeda, M. Koyama, and S. Ishii. Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning. *TPAMI*, 2018. 3, 4, 5, 12, 14
[33] A. Najafi, S.-i. Maeda, M. Koyama, and T. Miyato. Robustness to Adversarial Perturbations in Learning from Incomplete Data. *arXiv preprint arXiv:1905.13021*, May 2019. 2, 3
[34] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope.

IJCV, 42(3):145–175, 2001. 7, 15
[35] A. Oliver, A. Odena, C. Raffel, E. D. Cubuk, and I. J. Goodfellow. Realistic Evaluation of Deep SemiSupervised Learning Algorithms. In *NeurIPS*, 2018. 7
[36] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: a loss correction approach. In *CVPR*, 2017. 7
[37] S.-A. Rebuffi, S. Ehrhardt, K. Han, A. Vedaldi, and A. Zisserman. Semi supervised learning with scarce annotations. In *ICML*, 2019. 3
[38] D. Rolnick, A. Veit, S. J. Belongie, and N. Shavit. Deep learning is robust to massive label noise. *CoRR*,
abs/1705.10694, 2017. 7
[39] M. Sajjadi, M. Javanmardi, and T. Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In *NeurIPS*, 2016. 3
[40] P. Samangouei, M. Kabkab, and R. Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. *arXiv preprint arXiv:1805.06605*, 2018. 2
[41] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry. Adversarially Robust Generalization Requires More Data. In *NeurIPS*, 2018. 2, 4, 5, 20, 22
[42] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. *arXiv preprint arXiv:1710.10766*, 2017. 2
[43] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In *ICLR*, 2014. 1, 2
[44] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny images: a large dataset for non-parametric object and scene recognition. *TPAMI*, 2008. 7, 15
[45] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble adversarial training: Attacks and defenses. *arXiv preprint arXiv:1705.07204*, 2017. 9
[46] J. Uesato, B. O'Donoghue, A. v. d. Oord, and P. Kohli. Adversarial risk and the dangers of evaluating against weak attacks. In *ICML*, 2018. 2, 9, 16
[47] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. *arXiv preprint arXiv:1607.08022*, 2016. 14
[48] E. Wong, F. R. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. In *NeurIPS*,
2018. 5, 8
[49] Y. Wu and K. He. Group normalization. In *Proceedings of the European Conference on Computer Vision*
(ECCV), pages 3–19, 2018. 14
[50] Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, and Q. V. Le. Unsupervised Data Augmentation. *arXiv:1904.12848*,
2019. 3, 7, 14
[51] D. Yin, K. Ramchandran, and P. Bartlett. Rademacher complexity for adversarially robust generalization.

In *ICML*, 2018. 2
[52] S. Zagoruyko and N. Komodakis. Wide residual networks. *arXiv preprint arXiv:1605.07146*, 2016. 2, 12, 15
[53] R. Zhai, T. Cai, D. He, C. Dan, K. He, J. Hopcroft, and L. Wang. Adversarially Robust Generalization Just Requires More Unlabeled Data. *arXiv preprint arXiv:1906.00555*, Jun 2019. 2, 3
[54] H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan. Theoretically Principled Trade-off between Robustness and Accuracy. *arXiv:1901.08573*, 2019. 2, 3, 4, 5, 8, 9, 13, 14
[55] S. Zheng, Y. Song, T. Leung, and I. Goodfellow. Improving the Robustness of Deep Neural Networks via Stability Training. In *CVPR*, 2016. 8

## Overview

The appendices are organised as follows. Appendix A provides additional details on the experiments in Section 4. Appendix B details our implementation of VAT [32] adapted for L∞ adversarial robustness. Appendix C provides additional details on the 80m@N dataset generation procedure.

Appendix D details code release. Appendix E includes additional experiments for the adversarial evaluation of our trained models, as well as checks against gradient masking. Finally, we include the proof of Theorem 1 in Appendix G.

## A Experimental Details A.1 Implementation Notes

Model architecture. For all experiments, we use variants of wide residual networks (WRNs) [52].

In Section 4.1, we use a WRN of width 2 and depth 28 for SVHN and a WRN of width 8 and depth 28 for CIFAR-10. We explore increasing the depth of the network (while keeping width to be 8) to 34, 70 and 106 in Section 4.2.2.

Data preprocessing. We use standard data augmentation techniques for images. For CIFAR-10, 4-pixel padding is used before performing random crops of size 32x32 and random left-right flip. For SVHN, 4-pixel padding is also employed before random crops of size 32x32 followed by random color distortions.

Pseudocode. We provide pseudocode for our particular implementations of each UAT variant. To simplify notation, when writing (x, y) ∼ Um, the target y is always the fixed target pseudo-label
(which is unused in UAT-OT). Recall that these pseudo labels are obtained from a model trained on Sn alone. Lˆadvand LˆOT are the empirical estimates of the robust loss from [30] (as in UAT-FT) and L
OT respectively, as defined in the next section.

## Algorithm 1 Uat-Ot Update

Input: Weight hyperparameter λ, batch sizes bs and bu Sample bs labeled examples (xs, ys) ∼ Sn and bu unlabeled examples (xu, yu) ∼ Um Compute loss L = Lˆadv(xs, ys; θ) + λ( bs bu
)LˆOT (xu, yu; θ)
Update with gradient g = ∇θL

## Algorithm 2 Uat-Ft Update

Input: Batch sizes bs and bu Sample bs labeled examples (xs, ys) ∼ Sn and bu unlabeled examples (xu, yu) ∼ Um Merge x = [xs; xu]; y = [ys; yu]
Compute loss L = Lˆadv(x, y; θ)
Update with gradient g = ∇θL

## Algorithm 3 Uat++ Update

Input: Weight hyperparameter λ, batch size bs and bu Sample bs labeled examples (xs, ys) ∼ Sn and bu unlabeled examples (xu, yu) ∼ Um Merge x = [xs; xu]; y = [ys; yu]
Compute loss L = Lˆadv(x, y; θ) + λLˆOT (x, y; θ)
Update with gradient g = ∇θL
Loss implementations. In the above, the empirical estimates of the losses are defined as follows:

$$\hat{\mathcal{L}}^{adv}(\mathbf{x},\mathbf{y};\theta)=\frac{1}{|(\mathbf{x},\mathbf{y})|}\sum_{i=1}^{|(\mathbf{x},\mathbf{y})|}\sup_{x_{i}^{\prime}\in N_{\epsilon}(\mathbf{x}_{i})}\texttt{xent}(\mathbf{y}_{i},p_{\theta}(\cdot|x_{i}^{\prime}))$$ $$\hat{\mathcal{L}}^{OT}(\mathbf{x},\mathbf{y};\theta)=\frac{1}{|(\mathbf{x},\mathbf{y})|}\sum_{i=1}^{|(\mathbf{x},\mathbf{y})|}\sup_{x_{i}^{\prime}\in N_{\epsilon}(\mathbf{x}_{i})}\mathcal{D}(p_{\hat{\theta}}(\cdot|\mathbf{x}_{i}),p_{\theta}(\cdot|x_{i}^{\prime}))$$

We always approximate each maximization with 10-steps of PGD, as described in Appendix A.3. For LˆOT , there are two implementational details of the attack which are not obvious from the pseudocode.

- When computing LˆOT in UAT-OT, we solve the adversarial optimization with a "hardlabel" rather than "soft-label" attack. That is, rather than maximizing the KL directly, we take the hard label yˆ = arg maxy pθ(y|x) and run a PGD attack using yˆ as the label. We suspect this is because when pθ(y|x) is not completely one-hot, we are maximizing a convex objective. There are (at least) two issues due to this. First, the gradient of the KL, evaluated at x, is 0, since p(y|x) is the global minimum. Second, if the random initialization x 0 causes p(ˆy|x 0) > p(ˆy|x), then the gradient will encourage increasing p(ˆy|x 0), rather than decreasing it. While previous work in [54] finds that initializing to a random perturbation of x allows PGD to effectively maximize the KL, using hard labels worked better in our experiments.

- When computing LˆOT in UAT++, we reuse the adversarial example computed to maximize Lˆadv, for computational efficiency.

In both cases, the model loss is still a KL divergence.

Loss weights. For all experiments, we used λ = 5 for both UAT-OT and UAT++. We fixed these values based on early experiments.

Differences with distribution shift. For our off-distribution experiment in Section 4.2.2, we make two changes to accomodate distribution shift. First, we compute the loss on the labeled and unlabeled examples using separate forward passes through the network. Without batch norm, this has no effect on the computation. With batch norm, we observe this helps slightly, perhaps because the off-distribution unlabeled data degrades the local batch statistics. Second, we downweight the loss of unlabeled examples by a factor of bs/bu (this corresponds to averaging the loss across examples within each separate batch, rather than summing). We suspect that while label noise has little effect, distribution shift degrades performance, and so focusing more on in-distribution examples is helpful.

Batch sizes. Throughout Section 4.1, we use batch sizes proportional to the dataset size. In other words, given labeled and unlabeled datasets Sn and Um, for a total batch size of B = 128, we use bs = BN/(N + M) and bu = BM/(N + M). This ensures we perform an equal number of passes through each dataset, and helps avoid overfitting the (small) labeled dataset. For our high data regime experiment in Section 4.2.2, we use fixed bs = 512 and bu = 4096.

Optimization. We use the SGD with momentum optimizer for all training jobs. For all training jobs, we use weight decay (L2 regularization) weighted by 5 × 10−4.

In Section 4.1.1, we use a total batch size of 128, for 25K steps with an initial learning rate of 0.2 which is decreased by a factor 10 at iterations 15K, 18K and 20K. This schedule was fine-tuned using the validation set. In Section 4.2, we use the same learning rate schedule of [54]'s public repository is used for CIFAR-10, i.e. initiate with 0.2, then dividing by 10 after 15K and 22K steps.

## A.2 Negative Results And Observations

We note several observations we made while running experiments. Note that these are much less carefully examined than the results reported in the main paper. Indeed, we suspect that some of these observations may be specific to our specific training setup. However, we include these as we believe they may nonetheless be helpful for future researchers:
- We merge (pseudo)-labeled and unlabeled batches when possible, since this improved robust accuracy, particularly in the small-data regime (when using smaller batch sizes) We suspect this is due to more reliable local batch statistics in batch normalization, since with small batch sizes, the labeled batch can be quite small.

- We found learning rate schedules to be particularly important. For example, in Table 1, the primary difference between our reimplementation of adversarial training, and the implementation in [30] is we use the significantly shorter schedule from [54], which improves robust accuracy by ~3%.

- With our current choices for λ, we notice a strong regularizing effect from LˆOT . While UAT-FT reaches nearly 100% train robust accuracy by the end of training, the same schedule used with UAT-OT or UAT++ stays below 80% train robust accuracy.

- When using LˆOT on unsupervised data, we noticed it was necessary to either use fixed parameters ˆθ for computing the target prediction, or to also use fixed targets on the unlabeled data. When both these are missing, the model would learn to predict a uniform class distribution for images in the unsupervised dataset. While such models achieve high training accuracy, test set accuracy stays close to random, even on unperturbed images. On labeled data, both versions with and without backpropagating into target predictions work fine, as reported in [54].

- We tried combining UAT with the recent semi-supervised UDA method [50] but did not see significant improvements in adversarial accuracy.

- We experimented with alternative normalization strategies such as InstanceNorm [47] and GroupNorm [49], but observed slightly worse results.

## A.3 Projected Gradient Descent (Pgd) Details

We provide additional details on our untargeted PGD attack, which we use for adversarial training.

The untargeted attack optimizes the margin loss objective proposed in [9]

$$J_{\theta}^{\mathrm{adv}}(x)=Z(x,\theta)_{y}-\operatorname*{max}_{i\neq y}Z(x,\theta)_{i}\;,$$

where Z(*x, θ*)i denotes the logit for class i, predicted by model θ on input x, and y denotes the true label. The margin loss is negative if and only if x is misclassified.

We optimize this objective with projected gradient descent [26, 30] using the Adam optimizer [24].

Consistent with prior work (e.g. [9, 29]), we find these modifications to improve attack convergence speed when compared to using the negative cross-entropy loss or vanilla gradient updates. During training, we perform 10 steps of optimization, as in [54].

## B Implementation Note On Vat Baseline Implementation

Recall that for UAT-OT, we use the loss introduced in [32] and also used in [54]

$${\mathcal{L}}_{\mathrm{unsup}}^{O T}(\theta)=\operatorname*{\mathbb{E}}_{x\sim P(X)}\operatorname*{sup}_{x^{\prime}\in{\mathcal{N}}_{\epsilon}(x)}{\mathcal{D}}(p_{\hat{\theta}}(.|x),p_{\theta}(.|x^{\prime})),$$
0)), (3)
where D is the Kullback-Leibler divergence, and ˆθ indicates a fixed copy of the parameters θ in order to stop the gradients from propagating.

In practice, computing the optimal perturbation x 0(in supx0∈N(x)) cannot be achieved in closed form, hence approximations have to be proposed. In [32], the authors start from a second order Taylor approximation of D(pθˆ(.|x), pθ(.|x 0)), then use a combination of finite differences to efficiently approximate the Hessian and power iteration method to find an estimate of the optimal perturbation.

Their end goal being *standard generalization*, they observe that only one iteration of the power method is sufficient for good performance. This specific procedure (Hessian approximation and eigenvector estimation though power method) is specifically tailored to the case where the chosen specification for the adversary corresponds to the L2 ball: N(x) = {x 0: kx 0 − xk2 ≤ }, whereas we focus on the L∞ ball type of constraints.

$$({\mathfrak{I}})$$

For fair comparison, we adapt their algorithm to the L∞ ball by replacing the previously mentioned procedure by one step of FGSM with learning rate equal to .

UAT-OT mainly differs from VAT in the fact that we instead use 10 steps of PGD to estimate x 0.

Looking at Figure 1 in the main paper, UAT-OT clearly outperforms VAT, indicating that when it comes to adversarial generalization, one has to use a stronger adversary to generate the perturbation.

## C Dataset Details

We detail the procedure used to generate the 80m@N datasets, used for experiments in Section 4.2.

To remove exact and near duplicate images, we follow [15] and use the GIST image descriptors [34]
provided with the 80 Million Tiny Images dataset [44]. For every image, we compute the L2 distance to its nearest neighbor in the CIFAR-10 test set, as measured in GIST feature space, and remove all images within distance 0.28, totalling roughly one million images. Following this, we manually checked for near duplicates, by random sampling and visualizing L2 nearest neighbors. In our initial version of this paper, we removed only exact duplicates, which produced robust accuracies roughly 0.5 - 2% higher across all experiments.

For filtering, we use a WRN-28-10 [52] model trained on CIFAR-10, which achieves 96.0% accuracy on the CIFAR-10 test set. Following the procedure used for the original CIFAR-10 dataset [25], for each class in CIFAR-10, we use hyponyms of the class name, based on the Wordnet hierarchy [31].

This leaves roughly 2 million images remaining, though the class distribution is highly non-uniform
(of these, 1 million correspond to the "dog" class, while just 50000 correspond to "deer" or "frog").

For each image, we record the probability assigned by the pretrained model to the associated class.

We restrictto images with associated probability over 0.5, and take the top N/10 images per class.

For the 80m@500K dataset, some classes contain less than 50K such examples. In this case, we randomly duplicate examples, to maintain class balance. Finally, we note that although we use a simple procedure here, and did not experiment with alternatives, we believe developing better procedures to exploit uncurated data is an important and underexplored research direction.

## D Code Release

Example usage of our best performing model (WRN-106 trained with UAT++ on 80m@200K) as well as all the 80m@N datasets we used to train our models can be found on our github repository.2

## E Adversarial Evaluation Details E.1 Multitargeted Attack Evaluation

We evaluate using the MultiTargetedattack proposed in [19], which we have found to be significantly stronger than commonly used PGD attacks, at the cost of increased computation. The MultiTargeted implementation is equivalent to the untargeted attack described above, but rather than performing a single optimization, instead runs a targeted attack against each possible class, and returns the image which minimizes the untargeted loss. The targeted margin loss is

J
$I^{\mathrm{adv}}(x)=-1$
θ(x) = Z(x, θ)y − Z(*x, θ*)t ,
where t is the target class. We use 200 steps with 20 random restarts for each class.

We have found the MultiTargeted attack to reliably outperform untargeted PGD attacks, which in turn reliably outperform FGSM20. In ensuring the strongest results for untargeted PGD, we found that using 200 steps with 20 random restarts slightly outperforms 100 steps with 1000 random restarts, and hence report results using the former. For example, for our strongest model, trained on the 80m@200K dataset, the FGSM20adversarial accuracy is 63.65%, the untargeted PGD adversarial accuracy is 61.10%, and the MultiTargeted adversarial accuracy is 56.30%.

2https://github.com/deepmind/deepmind-research/tree/master/unsupervised_
adversarial_training

## E.2 Spsa Evaluation

As an additional check against gradient masking [1, 46], we run a gradient-free attack, SPSA [46],
against our strongest model, UAT++ trained on the 80m@200K unsupervised dataset. For SPSA,
we use a batch size of 8192 with 40 iterations. We obtain 64.9% adversarial accuracy, similar to the 61.1% obtained by untargeted PGD. We further observe in Figure 4 that SPSA and PGD reliably converge to similar loss values, and that SPSA rarely outperforms PGD. This provides additional evidence that the model's strong performance is not due to gradient masking.

![15_image_0.png](15_image_0.png)

J
adv θ(x) using PGD
Figure 4: **Analysis of gradient masking with SPSA**: We compare the final values of the margin loss across different images, between PGD and SPSA. Each point represents a single image, which is misclassified when J
adv θ(x) < 0. Overall, we find that SPSA and PGD converge to similarly adversarial perturbations (points close to the line y = x). We observe relatively few images where SPSA outperformed PGD (below the line, shown in red). The upward bend in the dots to the left of
−3 are an artifact due to the fact that we terminate the SPSA attack early, once we find any image with margin loss below −3.

## E.3 Loss Landscape Analysis

As another check against gradient masking, we look at the adversarial loss landscape from our strongest model. We examine the loss surfaces for four images where the MultiTargeted attack succeeded, but untargeted PGD attack, with 200 steps and 20 restarts, did not. Figure 5 shows the untargeted adversarial loss (optimized by PGD) around the nominal image from CIFAR-10. In these loss landscapes, we vary the input along a linear space defined by the worse perturbations found by PGD and a random direction. The u and v axes represent the magnitude of the perturbation added in each of these directions respectively and the z axis represents the loss. For figures on the right hand side of Figure 5, they show a top-view of the loss landscape and indicates that a large portion of L∞ ball around the nominal image pushes the PGD solution towards the right (rather than the bottom). We observe that the loss landscape is rather smooth, which provides (weak) additional evidence that the strong performance is not due to gradient masking.

## E.4 Attack Convergence Analysis

As another check against gradient masking, we analyzed the convergence of PGD. Figure 6 shows convergence of untargeted PGD across different random restarts for our strongest model, though we also observe similar patterns across other models. We observed that on randomly selected images, PGD quickly converges, and that the final loss values across random restarts are tightly clustered, indicating PGD likely converges to near-optimal perturbations. Figure 6a shows randomly selected images, and is consistent with what we observe across images where MultiTargeted and PGD agree. In the fraction of cases where MultiTargeted succeeded but PGD did not, we can find evidence of gradient masking on some images through random restarts. In Figure 6b, there are two images where the final loss varies across different random restarts.

## F Additional Experimental Results

L2 **robustness.** We ran several short experiments to ensure our results hold for L2 in addition to L∞
robustness. We use 4K labeled and 32K unlabeled examples. On CIFAR-10 at L2 radius  = 0.87, which encloses the L∞  = 4/255 ball, the purely supervised model achieves 32.7% robust accuracy, the supervised oracle achieves 53.9%, and UAT almost matches this, with 55.2% robust accuracy.

This represents a 21% absolute gain from using unlabeled data, which captures over 90% of the oracle improvement, without using additional labels. We observe similar results for  = 0.435, which encloses the L∞  = 2/255 ball, of 47.3% / 70.3% / 66.3% for the purely supervised baseline, supervised oracle, and UAT, respectively.

Number of necessary labels. To study the minimum number of labels required while maintaining robustness, we also train CIFAR-10 models using fewer labels. In the body of the paper, we report that with 4K labels (and 32K unlabeled examples), UAT achieves 54.1% robust accuracy, compared to 55.5% for the supervised oracle which uses 36K labeled examples. We also trained models using 2K and 1K labels, which yield robust accuracies of 51.9% and 47.7% respectively. Thus, there is some loss of robustness - with 4K labels, UAT almost exactly matches the performance of the supervised oracle, with only a 1.4% gap, whereas with 2K and 1K labels, the gap is larger. However, even in this regime, UAT still achieves significant adversarial robustness.

## G Proof Of Theorem 1 G.1 Preliminaries

We first provide the following two concentration inequalities which we will use to bound our main
quantities of interest.
Lemma 3 (Concentration of χ-squared distribution). Let X ∼ N (0, σ2In)*. Then, provided* α
2 >
2nσ2,
$$\mathbb{P}(\|X\|^{2}\geq\alpha^{2})\leq e^{-\alpha}$$
2/(20σ
2).
$$\mathbf{f}$$
Proof. The result follows from application of Lemma 1 from [28].

Lemma 4. Let X ∼ N (0, σ2Im) in R
m*. Then*

$$\boxed{\bot}$$
$$\mathbb{P}({\frac{1}{m}}\|X\|_{1}\geq a)\leq2^{m}\exp-{\frac{m a^{2}}{2\sigma^{2}}}$$

Proof. Forming the Chernoff bound with t =
ma σ2 , we have:

$$\mathbb{P}(\frac{1}{m}\|X\|_{1}\geq a)\leq\exp-\frac{ma^{2}}{\sigma^{2}}\,\mathbb{E}[\exp\frac{a}{\sigma^{2}}\|X\|_{1}]$$ $$=\exp-\frac{ma^{2}}{\sigma^{2}}\left(\mathbb{E}[\exp\frac{a}{\sigma^{2}}|X_{1}|]\right)^{m}$$ $$=\exp-\frac{ma^{2}}{\sigma^{2}}\left(\exp\frac{a^{2}}{2\sigma^{2}}(1+\operatorname{erf}\frac{a}{\sigma\sqrt{2}})\right)^{m}$$ $$=\exp-\frac{ma^{2}}{\sigma^{2}}\exp\frac{ma^{2}}{2\sigma^{2}}\left((1+\operatorname{erf}\frac{a}{\sigma\sqrt{2}})\right)^{m}$$ $$\leq2^{m}\exp-\frac{ma^{2}}{2\sigma^{2}}$$

## G.2 Main Proof

To bound the robustness, there are two main quantities of interest. First, we need to bound the norm of z =
1 m Pm i=1 yˆixi, which controls the smoothness of the classifier (Lemma 5). Second, we need to bound the inner product h*z, θ*∗i, which controls how well the classifier fits the data (Lemma 6).

The main difficulty is that Pm i=1 yˆixiis not Gaussian distributed. In particular, while Pm i=1 yixi follows a Gaussian distribution, our quantity of interest does not, due to the dependence of yˆi on xi.

Lemma 5. *Given a* (θ
?, σ) *Gaussian model in* R
d*, let* h : R
d → {−1, +1} *be any classifier.*
If z =1m Pm i=1 yˆixiis the sample mean vector of m *i.i.d. samples based on predicted classes* yˆi = h(xi)*, then we have*

$$\mathbb{P}\left(\|{\overline{{z}}}\|_{2}\geq(1+c)\|\theta^{*}\|_{2}+2\sigma{\sqrt{\frac{d}{m}}}\right)\leq e^{-6{\sqrt{d}}/5},$$

with c =
√
20σ kθ
∗k q√d m + log 2.

Proof. We have
we have  $$\left\|\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}x_{i}\right\|_{2}=\left\|\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}(y_{i}\theta^{*}+z_{i})\right\|_{2}\leq\frac{\|\theta^{*}\|}{m}\sum_{i=1}^{m}\hat{y}_{i}y_{i}+\left\|\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}z_{i}\right\|_{2}$$ $$\leq\|\theta^{*}\|+\left\|\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}z_{i}\right\|_{2},$$
where zi ∼ N (0, σ2I). We therefore have P   1 m Xm i=1 yˆixi 2 ≥ t ! ≤ P   1 m Xm i=1 yˆizi 2 ≥ t − kθ ∗k ! ≤ P  [ s1=±1,...,sm=±1  1 m Xm i=1 sizi 2 ≥ t − kθ ∗k ! ≤ X s P  1 m  Xm i=1 sizi 2 ≥ t − kθ ∗k ! Observe that Pm i=1 sizi ∼ N (0, mσ2). Now, using the concentration of measure result, whenever

t − kθ
∗k ≥ q2m σ we have:
$$\mathbb{P}\left(\left\|\sum_{i=1}^{m}s_{i}z_{i}\right\|_{2}^{2}\geq m^{2}(t-\|\theta^{*}\|)^{2}\right)\leq e^{-m^{2}(t-\|\theta\|)^{2}/(20m\sigma^{2})}=e^{-m(t-\|\theta\|)^{2}/(20\sigma^{2})}.$$  Hence, we obtain  $$\mathbb{P}\left(\left\|\frac{1}{m}\sum_{i=1}^{m}\hat{g}_{i}x_{i}\right\|_{2}\geq t\right)\leq2^{m}e^{-m(t-\|\theta^{*}\|)^{2}/(20\sigma^{2})}.$$  Let $t=(1+c)\|\theta^{*}\|+2\sigma\sqrt{\frac{d}{m}}$. Then, we have 

$\left|\left|\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}x_{i}\right|\right|_{2}\geq t\right)\leq2^{m}e^{-m\left(c\|\theta^{*}\|+2\sigma\sqrt{\frac{d}{m}}\right)^{2}/(20\sigma^{2})}$  $$\leq2^{m}e^{-m\left(c^{2}\|\theta^{*}\|_{2}^{2}/(20\sigma^{2})\right)}e^{-m4\sigma^{2}\frac{d}{m}/(20\sigma^{2})}$$ $$=2^{m}e^{-m\left(c^{2}\|\theta^{*}\|_{2}^{2}/(20\sigma^{2})\right)}e^{-d/5}$$

 Now let $c=\frac{\sqrt{20}\sigma}{\|\theta^*\|_2}\sqrt{\frac{\sqrt{d}}{m}+\log2}.$ Then  ... 
m + log 2. Then, the above probability is given by
$$\begin{array}{l}{{e^{-i\pi/\sqrt{\pi}}\cdot m}}\\ {{{2^{m}e^{-m\left(\frac{\sqrt{d}}{m}+\log2\right)}e^{-d/5}=e^{-\sqrt{d}}e^{-d/5}\leq e^{-\sqrt{d}}e^{-\sqrt{d}/5}=e^{-6\sqrt{d}/5}.}}\end{array}$$
Lemma 6. Under the conditions of Lemma *5, let* p = E[I[h(x) = y]] *denote the accuracy of* classifier h*. Then we have*

$$\mathbb{P}\left((\overline{{{\varepsilon}}},\theta^{*})\leq\left(\frac{7}{4}p-1\right)\|\theta^{*}\|^{2}-\sqrt{2}\|\theta^{*}\|\sigma\sqrt{\frac{\log(1/\delta)}{m}+\log2}\right)\leq(0.995)^{m p}+\delta.$$

Proof. We write

$$\mathbb{P}\left(\langle\hat{w},\theta^{*}\rangle\leq t\right)=\mathbb{P}\left(\langle\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}x_{i},\theta^{*}\rangle\leq t\right)=\mathbb{P}\left(\langle\frac{1}{m}\sum_{i=1}^{m}\hat{y}_{i}(y_{i}\theta^{*}+z_{i}),\theta^{*}\rangle\leq t\right),$$  we $z_{i}$ are $N(0,\sigma^{2}I)$. The expression inside the probability is equal to 
where zi are N (0, σ2I). The expression inside the probability is equal to

$${\frac{\|\theta^{*}\|_{2}^{2}}{m}}\sum_{i=1}^{m}{\hat{y}}_{i}y_{i}+{\frac{1}{m}}\sum_{i=1}^{m}\langle{\hat{y}}_{i}z_{i},\theta^{*}\rangle$$

We bound this expression from below with

$${\frac{\|\theta^{*}\|_{2}^{2}}{m}}\sum_{i=1}^{m}{\hat{y}}_{i}y_{i}-{\frac{1}{m}}\sum_{i=1}^{m}|\langle z_{i},\theta^{*}\rangle|$$

(That is, we consider the worst-case scenario where the random variables yˆi are given by the negative of the sign of hzi, θ∗i). We therefore get that

P (hw, θ ˆ ∗i ≤ t − t 0) ≤ P  kθ ∗k 2 2 m Xm i=1 yˆiyi − 1 m Xm i=1 |hzi, θ∗i| ≤ t − t 0 ! ≤ P  kθ ∗k 2 2 m Xm i=1 yˆiyi ≤ t ! + P  1 m Xm i=1 |hzi, θ∗i| ≥ t 0 ! = P  2kθ ∗k 2 2 m Xm i=1 I[yi = ˆyi] − kθ ∗k 2 2 ≤ t ! + P  1 m Xm i=1 |hzi, θ∗i| ≥ t 0 ! We treat the first term. Let t =74 p − 1kθ ∗k 2. The first probability term is hence given by P  Xm i=1 I[yi = ˆyi] ≤ 7 8 mp!≤ exp − mp 2 · 8 2 ≤ (0.995)mp. using a Chernoff bound.

We have the following concentration bound on the `1 norm of the Gaussian vectors U ∼
N (0, kθ
∗k
2σ
2):
$$\mathbb{P}\left(\frac{1}{m}\|U\|_{1}\geq t^{\prime}\right)\leq2^{m}\exp\left(-t^{\prime2}m/(2\|\theta^{*}\|^{2}\sigma^{2})\right),$$  **1.** We set $t^{\prime}=\sqrt{2}\|\theta^{*}\|\sigma\sqrt{\frac{\log(1/\delta)}{\sigma}+\log2}$, and when 
by applying Lemma 4. We set t
√2kθ
∗kσ m + log 2, and when plugging in the above formula, we obtain

$$\mathbb{P}\left({\frac{1}{m}}\sum_{i=1}^{m}|\langle z_{i},\theta^{*}\rangle|\geq t^{\prime}\right)\leq\delta.$$
$$\square$$

Hence, we obtain the desired bound.

We now use these results to achieve the final result in Theorem 1. In what follows, we assume:

- (x1, y1), . . . ,(xm, ym) are drawn i.i.d. from a (θ
?, σ) Gaussian model in R
d with mean norm kθ
?k2 =
√d
- h : R
d *→ {−*1, +1} is a base classifier with accuracy p > 34
, where p = E[I[h(x) = y]]
- z ∈ R
dis the sample mean vector z =
1 m Pm i=1 yˆixi, where yˆi = h(xi)
- wˆ ∈ R
dis the unit vector in the direction of z, i.e., wˆ = z/kzk2
- c denotes the constant in Lemma 5 Lemma 7. *Under these assumptions,*

P " hw, θ ˆ ?i ≤ 74 p − 1 √dm −pd + 2mσ2 log 2 (1 + c) √m + 2σ #

is bounded above by exp(−6
√d/5) + (0.995)mp + exp(−d/2σ 2).

Proof. By Lemma 6, we have

By Lemma 6, we have  $$\mathbb{P}\left[\left(\overline{z},\theta^{*}\right)\geq\left(\overline{\frac{7}{4}}p-1\right)\left\|\theta\right\|^{2}-\sqrt{2}\|\theta\|\sigma\sqrt{\frac{\log(1/\delta)}{m}+\log2}\right]\geq1-\left(0.995\right)^{mp}-\delta.$$  In $\mathbb{P}$, we have 

Further, by Lemma 5, we have

$$\mathbb{P}\left(\|{\hat{w}}\|_{2}\leq(1+c)\|\theta\|+2\sigma{\sqrt{\frac{d}{m}}}\right)\geq1-e^{-6{\sqrt{d}}/5},$$

Conditioning on both events with δ = exp(−d/2σ 2), the overall failure probability is bounded by exp(−6
√d/5) + (0.995)mp + exp(−d/2σ

$(2.995)^{mp}+\exp(-d/2\sigma^{2})$. Then, we have  $$\langle\hat{w},\theta^{\star}\rangle=\frac{\langle\overline{z},\theta^{\star}\rangle}{\|z\|_{2}}$$ $$\geq\frac{\left(\frac{7}{4}p-1\right)d-\sqrt{2d}\sigma\sqrt{\frac{\log^{1/s}}{m}+\log2}}{(1+c)\sqrt{d}+2\sigma\sqrt{\frac{d}{m}}}$$ $$=\frac{\left(\frac{7}{4}p-1\right)\sqrt{dm}-\sigma\sqrt{2}\sqrt{\log^{1/s}+m\log2}}{(1+c)\sqrt{m}+2\sigma}$$ $$=\frac{\left(\frac{7}{4}p-1\right)\sqrt{dm}-\sqrt{d+2\sigma^{2}m\log2}}{(1+c)\sqrt{m}+2\sigma}$$
For ease of reference, we provide a relevant lemma proved in [41].

Lemma 8 ([41]). *Assume a* (θ
?, σ)-Gaussian model. Let p ≥ 1, ε ≥ 0 *be robustness parameters,*
and let wˆ be a unit vector such that h*w, θ* ˆ
?i ≥ ε kwˆk
∗
p
., where k·k∗p is the dual norm of k·kp
. Then the linear classifier fwˆ has `
ε p
-robust classification error at most

$$\exp\!\left(-\frac{\left(\langle\hat{w},\theta^{\star}\rangle-\varepsilon\left\|\hat{w}\right\|_{p}^{\ast}\right)^{2}}{2\sigma^{2}}\right)\,.$$

Lemma 9. *With probability at least* 1 − [exp(−6
√d/5) + (0.995)mp + exp(−d/2σ 2)]*, the linear* classifier fwˆ has `
ε∞*-robust classification error at most* β if

$$\varepsilon\leq{\frac{1}{\sqrt{d}}}{\frac{\left({\frac{7}{4}}p-1\right){\sqrt{d m}}-{\sqrt{d+2\sigma^{2}m\log2}}}{(1+c){\sqrt{m+2\sigma}}}}-\sigma{\sqrt{\frac{2\log^{1/\beta}}{d}}}$$

Proof. We follow the approach used for Theorem 21 in [41]. Define

$$\alpha={\frac{\left({\frac{7}{4}}p-1\right){\sqrt{d m}}-{\sqrt{d+2\sigma^{2}m\log2}}}{(1+c){\sqrt{m}}+2\sigma}}$$
so that we can rewrite
$$\varepsilon\leq{\frac{1}{\sqrt{d}}}\alpha-\sigma{\sqrt{\frac{2\log{}^{1}/\beta}{d}}}$$

By Lemma 7, we have that h*w, θ* ˆ
?i ≥ α with probability at least 1 − [exp(−6
√d/5) + (0.995)mp +
exp(−d/2σ 2)].

$$\exp\Biggl{(}-\frac{(\langle\hat{w},\theta^{*}\rangle-\varepsilon\sqrt{d})^{2}}{2\sigma^{2}}\Biggr{)}\;.$$  Since  $$\langle\hat{w},\theta^{*}\rangle-\varepsilon\sqrt{d}\geq\alpha-\left(\frac{1}{\sqrt{d}}\alpha-\sigma\sqrt{\frac{2\log^{1/\beta}}{d}}\right)\sqrt{d}=\sigma\sqrt{2\log^{1/\beta}},$$  the robust classification error is bounded above by $\beta$, as desired.  
$$\square$$

Lemma 10. *Assume* σ ≤
1 32 d 1/4 and p > 0.99*. Then, with probability at least* 1−[exp(−6
√d/5) +
(0.995)mp + exp(−d/2σ 2)] the linear classifier fwˆ has `
ε p
-robust classification error at most 0.01 if

$m\;\geq\;\begin{cases}100&\text{for}\;\varepsilon\,\leq\,\frac{1}{4}d^{-1/4}\\ 256\,\varepsilon^2\sqrt{d}&\text{for}\;\frac{1}{4}d^{-1/4}\;\leq\;\varepsilon\;\leq\;\frac{1}{4}\end{cases}$ . 
Proof. We first apply Lemma 9 which gives a `
ε 0
∞-robust classification error at most β = 0.01 for

ε 0 =1 √d 74 p − 1 √dm −pd + 2σ 2m log 2 (1 + c) √m + 2σ− σ r2 log 1/β d ≥ 1 √d 74 p − 1 √dm −pd + 2σ 2m log 2 (1 + c) √m + 2σ− 1 8 d −1/4
The remainder is simple algebraic manipulation. First, we consider the case where ε ≤
1 4 d
−1/4. Using m = 100, we first bound

 Since manipulation: I now, we consider the cdf:  $\\c=\dfrac{\sqrt{20}\sigma}{\sqrt{d}}\sqrt{\frac{1}{m}d^{1/2}+\log2}\\\leq\dfrac{\sqrt{20}}{32}d^{-1/4}\sqrt{\frac{1}{100}d^{1/2}+\log2d^{1/2}}\\\leq\dfrac{\sqrt{20}}{32}d^{-1/4}\sqrt{d^{1/2}}\\\leq\dfrac{1}{5}$
The resulting robustness is

ε 0 ≥1 √d 74 p − 1 √dm −pd + 2σ 2m log 2 (1 + c) √m + 2σ− 1 8 d −1/4 = 74 p − 1 √m −p1 + 2σ 2md−1/2 log 2 (1 + c) √m + 2σ− 1 8 d −1/4 ≥ 7 10 √100 −p1 + 200σ 2d−1/2 log 2 6 5 √100 + 2σ− 1 8 d −1/4 ≥ 7 − q1 + 200 322 log 2 12 + 1 16 d 1/4− 1 8 d −1/4 ≥ 7 − q1 + 200 322 log 2 12 + 1 16 d 1/4− 1 8 d −1/4 ≥ 1 4 d −1/4 ≥ ε
Next, we consider the case where 14 d
−1/4 ≤ ε ≤
1 4
. We again bound c:

$$c={\frac{\sqrt{20}}{\sqrt{d}}}{\sqrt{\frac{\sqrt{d}}{m}}}+\log2$$ $$\leq{\frac{\sqrt{20}}{32}}d^{-1/4}{\sqrt{\frac{1}{16^{2}\varepsilon^{2}}}}+\log2$$ $$\leq{\frac{\sqrt{20}}{32}}d^{-1/4}{\sqrt{\frac{4^{2}{\sqrt{d}}}{16^{2}}}}+\log2$$ $$\leq{\frac{\sqrt{20}}{32}}{\sqrt{\frac{4^{2}}{16^{2}}+\frac{\log2}{\sqrt{d}}}}$$ $$\leq{\frac{1}{5}}$$

The resulting robustness is

ε 0 ≥1 √d 74 p − 1 √dm −pd + 2σ 2m log 2 (1 + c) √m + 2σ− 1 8 d −1/4 ≥ 1 √d 74 p − 1 √162ε 2d 3/2 − qd + 2 1 322 (162ε 2)d log 2 16(1 + c)εd1/4 + 1 16 d 1/4− 1 2 ε = 74 p − 1(16ε) − qd−1/2 + 162 322 log 2 2d−1/2ε 2 16(1 + c)ε + 1 16 − 1 2 ε ≥ 74 p − 1(16ε) − qd−1/2 +162 3224 2 log 2 2d−1/2 4(1 + c) + 1 16 − 1 2 ε ≥ 11.5ε − 4ε q1 + log 2 128 4 6 5 + 1 16 − 1 2 ε ≥ ε
as desired.
Corollary 11. Let (x0, y0) and (x1, y1), . . . ,(xm, ym) *be drawn i.i.d. from a* (θ
?, σ) Gaussian model with corruption parameter p and mean norm √d. Let wˆsup = y0x0*. Let* z ∈ R
d *be the sample* mean z =1m Pm i=1 yˆixi, where yˆi = fwˆsup
(xi)*. Let the UAT-FT estimator* wˆ ∈ R
d be the unit vector in the direction of z, i.e., wˆ = z/kzk2*. Assume* σ ≤
1 32 d 1/4*. Then, with probability at least* 1−[exp(−
6
√
d 5
)+ (0.996)m + exp(−
d 2σ2 )−2 exp(−d 8σ2+1) )], the linear classifier fwˆ has `
ε p
-robust classification error at most 0.01 if

$$m\ \geq\ \begin{cases}100&\quad\mathrm{for}\quad\varepsilon\ \leq\ \frac{1}{4}d^{-1/4}\\ 256\,\varepsilon^{2}\sqrt{d}&\quad\mathrm{for}\quad\frac{1}{4}d^{-1/4}\ \leq\ \varepsilon\ \leq\ \frac{1}{4}\end{cases}\ .$$

Using the given restriction on σ, we can invoke Corollary 19 from [41] with β = 0.01. Thus, with probability at least 1 − 2 exp(−d 8σ2+1) ), the classification error p of the base classifier fwˆsup is less than 0.01. Conditioning on this event, we can invoke Lemma 10 with m as given, which yields a robust classification error of fwˆ of at most 0.01, with probability at least 1 − [exp(−
6
√
d 5
) +
(0.996)m + exp(−
d 2σ2 ).

A union bound gives the desired total failure probability.

![22_image_0.png](22_image_0.png)

Figure 5: Adversarial loss landscapes around the nominal images. It is generated by varying the input to the model, starting from the original input image toward either the worst attack found using PGD (u direction) or the one found using a random direction (v direction). For the figures on the left hand side, the z axis represents the loss. For both panels, the diamond-shape represents the projected Lo ball of size € = 8/255 around the nominal image.

![23_image_0.png](23_image_0.png)

Figure 6: Convergence of PGD. Each plot shows the convergence of the adversary loss on the same image, across 20 random restarts. On randomly sampled images (top), the loss converges to tightly clustered values. On images where PGD did not find optimal perturbations (bottom), we observe variation in perturbation strength across different restarts for two images in the bottom row.